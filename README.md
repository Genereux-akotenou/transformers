### TRANSFORMERS 

We try to implement those papers for Seq2Seq architecture tasks: 
- [Attention Is All You Need](./paper/1706.03762v7.pdf)
- [Leave No Context Behind:Efficient Infinite Context Transformers with Infini-attention](./paper/2404.07143v2.pdf)

We also add class to support hugginface library format
- huggin-face

###### Objectives: 
- Try transformers on large context input (like entire genome)